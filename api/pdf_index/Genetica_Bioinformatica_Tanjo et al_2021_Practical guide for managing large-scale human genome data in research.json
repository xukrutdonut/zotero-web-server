{
  "pdfPath": "/home/arkantu/Documentos/Zotero Biblioteca/Genetica/Bioinformatica/Tanjo et al_2021_Practical guide for managing large-scale human genome data in research.pdf",
  "fileName": "Tanjo et al_2021_Practical guide for managing large-scale human genome data in research.pdf",
  "relativePath": "Genetica/Bioinformatica/Tanjo et al_2021_Practical guide for managing large-scale human genome data in research.pdf",
  "text": "Journal of Human Genetics\n                                    https://doi.org/10.1038/s10038-020-00862-1\n\n                                        REVIEW ARTICLE\n\n\n\n                                    Practical guide for managing large-scale human genome data in\n                                    research\n                                                         1                       2\n                                    Tomoya Tanjo             ●\n                                                                 Yosuke Kawai        ●\n                                                                                         Katsushi Tokunaga2 Osamu Ogasawara3 Masao Nagasaki\n                                                                                                            ●                      ●\n                                                                                                                                                         4,5\n\n\n\n                                    Received: 24 August 2020 / Revised: 8 October 2020 / Accepted: 11 October 2020\n                                    © The Author(s) 2020. This article is published with open access\n\n\n                                    Abstract\n                                    Studies in human genetics deal with a plethora of human genome sequencing data that are generated from specimens as well\n                                    as available on public domains. With the development of various bioinformatics applications, maintaining the productivity\n                                    of research, managing human genome data, and analyzing downstream data is essential. This review aims to guide struggling\n                                    researchers to process and analyze these large-scale genomic data to extract relevant information for improved downstream\n                                    analyses. Here, we discuss worldwide human genome projects that could be integrated into any data for improved analysis.\n                                    Obtaining human whole-genome sequencing data from both data stores and processes is costly; therefore, we focus on the\n1234567890();,:\n                  1234567890();,:\n\n\n\n\n                                    development of data format and software that manipulate whole-genome sequencing. Once the sequencing is complete and\n                                    its format and data processing tools are selected, a computational platform is required. For the platform, we describe a multi-\n                                    cloud strategy that balances between cost, performance, and customizability. A good quality published research relies on\n                                    data reproducibility to ensure quality results, reusability for applications to other datasets, as well as scalability for the future\n                                    increase of datasets. To solve these, we describe several key technologies developed in computer science, including\n                                    workﬂow engine. We also discuss the ethical guidelines inevitable for human genomic data analysis that differ from model\n                                    organisms. Finally, the future ideal perspective of data processing and analysis is summarized.\n\n\n\n                                    Introduction                                                            development of various bioinformatics tools, maintaining\n                                                                                                            the productivity of research, managing human genome data,\n                                    In human genetics, advancements in next-generation                      and analyzing downstream data is essential. This review\n                                    sequencing technology have enabled population-scale                     aims to guide researchers in human genetics to process and\n                                    sequencing from just one sequencer and allowed sharing                  analyze these large-scale genomic data to extract relevant\n                                    millions of human genome sequencing data from publicly                  information for improved downstream analyses in their\n                                    archived data including privacy-protected ones. With the                speciﬁc research domains.\n                                                                                                               Here, in each section, we answer the ﬁve inevitable\n                                                                                                            questions for human genome data processing and analysis:\n                                    * Osamu Ogasawara                                                       (i) what kind of large-scale human genome projects are\n                                      oogasawa@nig.ac.jp                                                    underway and available from data sharing? (ii) how to store\n                                    * Masao Nagasaki                                                        and analyze human genome data efﬁciently? (iii) what kind\n                                      nagasaki@genome.med.kyoto-u.ac.jp                                     of computational platforms are used to store and analyze\n                                                                                                            human genome data? (iv) how to maintain reproducibility,\n                                    1\n                                         National Institute of Informatics, Tokyo 101-8430, Japan           portability, and scalability in genome data analysis, and\n                                    2\n                                         Genome Medical Science Project, National Center for Global         why is it important? (v) which policy should be followed to\n                                         Health and Medicine, Tokyo 162-8655, Japan                         handle human genome data?\n                                    3\n                                         The Bioinformation and DDBJ Center, National Institute of             In “What kind of large-scale human genome projects are\n                                         Genetics, Mishima, Shizuoka 411-8540, Japan                        underway and available from data sharing?” section, we\n                                    4\n                                         Center for the Promotion of Interdisciplinary Education and        inform large-scale human genomic studies in worldwide\n                                         Research, Kyoto University, Sakyo-ku, Kyoto 606-8507, Japan        and how the data produced in these studies are sharing. Lots\n                                    5\n                                         Center for Genomic Medicine, Kyoto University Graduate School      of effort and cost are inevitable for storing and processing\n                                         of Medicine, Sakyo-ku, Kyoto 606-8507, Japan                       the human genomic data obtained by whole-genome\n\f                                                                                                                  T. Tanjo et al.\n\n\nsequencing (WGS). Therefore, in “How to store and ana-           analyze individual genotypes on a population scale. In this\nlyze human genome data efﬁciently?” section, we focus on         project, single nucleotide polymorphisms (SNPs) repre-\nthe development of data format and software that manip-          senting human genetic diversity were discovered and gen-\nulate WGS including hardware-based acceleration.                 otyped using SNP genotyping array technology, which was\n   Once the sequencing is complete and its format and data       popular at the time. In phase 1 study, the project completed\nprocessing tools are ready, a computational platform must        genome-wide genotyping of 269 individuals from 4 popu-\nbe selected, as discussed in “What kind of computational         lations. Finally, the analysis was extended to 1184 indivi-\nplatforms are used to store and analyze human genome             duals from 11 populations in phase 3 study. This was the\ndata?” section. For the platform, we recommend a multi-          ﬁrst study that revealed the structure of linkage dis-\ncloud strategy for balancing cost, performance, and custo-       equilibrium in human genome across the populations. The\nmizability. A high-quality published research relies on data     International 1000 Genomes Project is a successor to the\nreproducibility to ensure quality results, reusability for       HapMap project. This study aimed to comprehensively\napplications to other datasets, as well as scalability for the   elucidate the genetic diversity of human populations by\nfuture increase of datasets. “How to maintain reproduci-         utilizing next-generation sequencers, which was being put\nbility, portability, and scalability in genome data analysis,    to practical use at the time. In phase 1 study, whole gen-\nand why is it important?” section describes the method to        omes of 1092 individuals from 14 populations were\nsolve these demands using several key technologies, such as      sequenced by next-generation sequencers. The analysis\ncontainer technology, workﬂow description languages, and         eventually expanded to 2,504 individuals from 26 popula-\nworkﬂow engines. The ethical guidelines inevitable for           tions in phase 3 study [3], and then continued to incorporate\nhuman genomic data analysis that differ from model               new technologies, such as 3rd generation long leads\norganisms are discussed in “Which policy should be fol-          sequencers [4]. Importantly, all data and derivatives from\nlowed to handle human genome data?” section. Finally, the        the above-mentioned genome studies are available with\nfuture ideal perspective of human genome data processing         open access data sharing policy.\nand analysis in human genetics are discussed.                        Therefore, these data are not only used as summary\n                                                                 statistics, e.g., a catalog of allele frequencies of SNPs in\n                                                                 populations, but also used as individual-level information,\nWhat kind of large-scale human genome                            e.g., a whole-genome reference panel, which contains\nprojects are underway and available from                         individual genotype information for whole-genome regions,\ndata sharing?                                                    especially useful for genotype imputation to SNP geno-\n                                                                 typing arrays, e.g., Japonica Array [5]. Open access policy\nSeveral early collaborative large-scale human genome             also has the advantage of being used by many researchers.\nanalyses have been conducted worldwide. The Human                The data from the International 1000 Genomes Project has\nGenome Project (HGP) [1] is one of the largest and most          contributed to the development of a variety of NGS tools.\nsuccessful international collaborations in genome science.       Currently, common tools for NGS data analysis, e.g., bwa\nResearchers in institutes throughout the world contributed       [6] and de-facto standard formats, e.g., Sequence Align-\nto sequence all the bases in the human genome and                ment/Map (SAM), BAM, and VCF [7], have been devel-\nassembled them to construct one human reference assem-           oped in the International 1000 Genomes Project. In\nbly, followed by attempts to catalog genes hidden in the         addition, the genomic data are widely distributed under the\nhuman reference assembly. The assembly is the major              open access policy though various computational platforms,\nachievement of HGP, and the reference genome data is             e.g., high-performance computing (HPC) system of the\nfreely accessible from a very early phase. The Genome            National Institute of Genetics (NIG) in Japan and public\nReference Consortium (GRC) has taken over updating and           cloud services. These efforts also ease the reusability by\nmaintaining the assembly of human reference genome, and          researchers.\nthe updated versions of the human genome assembly are                Several present large-scale human genome analyses have\ncommonly used by researchers around the world. Nowa-             shifted toward understanding the relationship between\ndays, all researchers depend on the coordinate of the human      genotypes and phenotypes, e.g., diseases and traits. Of\nreference assembly from GRC. Therefore, the HGP study            these, cohort studies with biobanking play a key role, and\ninitially exempliﬁed the importance of data sharing in           many of these are prospective cohort studies of residents of\ngenome science.                                                  a speciﬁc region or country (Table 1) [8–28]. The DNA\n   After the great success of HGP, the human genome study        materials in the biobank allow us to measure the status of\nhas shifted toward studying the diversity of human gen-          the entire genome sequence, e.g., SNP genotyping array or\nomes. The HapMap Project [2] was one of the early large-         WGS, under the informed consent of participants. The\nscale population genomics studies used to systematically         genomic information and phenotypes collected in the cohort\n\fTable 1 Large-scale cohort studies with genomic information\nProject                              Description                                                                       Website                                                            Country         Reference\n\nHuman Genome Project (HGP)           The Initial sequencing program of the human genome                                https://www.genome.gov/human-genome-project                        International   [1]\nInternational HapMap Project         Study of the common pattern of human genetic variation using SNP array            https://www.genome.gov/10001688/international-hapmap-project       International   [2]\n1000 Genomes Project                 Determining the human genetic variation by means of whole-genome                  https://www.internationalgenome.org                                International   [3]\n                                     sequencing in population scale\nHuman Genome Diversity Project       Biological samples and genetic data collection from different population groups   https://www.hagsc.org/hgdp/                                        International   [8]\n                                     throughout the world\nSimon Genome Diversity Project       Whole-genome sequencing project of diverse human populations                      https://docs.cancergenomicscloud.org/v1.0/docs/simons-genome-      International   [9]\n                                                                                                                       diversity-project-sgdp-dataset\nGenome Asia 100k                   WGS-based genome study of people in South and East Asia                             https://genomeasia100k.org/                                        International   [10]\nUK Biobank                         Biobank study involving 500,000 residents in the UK                                 https://www.ukbiobank.ac.uk                                        UK              [11]\nGenomics England                   WGS-based genome study of patient with rare disease and their families and          https://www.genomicsengland.co.uk/                                 UK              [12]\n                                   cancer patients in England\nFinnGen                            Nationwide biobank and genome cohort study in Finland                               https://www.ﬁnngen.ﬁ/en                                            Finnland        [13]\nTohoku Medical Megabank Project Biobank and genome cohort study for local area (north-east region) in Japan            https://www.megabank.tohoku.ac.jp/english                          Japan           [14, 15]\nBiobank Japan                      Nationwide patient-based biobank and genome cohort study in Japan                   https://biobankjp.org/english/index.html                           Japan           [16]\nTrans-Omics for Precision Medicine A genomic medicine research project to perform omics analysis pre-existing          https://www.nhlbiwgs.org                                           USA             [17]\n(TOPMed)                           cohort samples\nBioMe Biobank                      Electronic health record-linked biobank of patients from the Mount Sinai            https://icahn.mssm.edu/research/ipm/programs/biome-biobank         USA             [18]\n                                   Healthcare System\nMichigan Genomics Initiative       Electronic health record-linked biobank of patients from the University of          https://precisionhealth.umich.edu/our-research/michigangenomics/   USA             [19]\n                                   Michigan Health System\n                                                                                                                                                                                                                      Practical guide for managing large-scale human genome data in research\n\n\n\n\nBioVU                              Repository of DNA samples and genetic information in Vanderbilt University          https://victr.vumc.org/biovu-description/                          USA             [20]\n                                   Medical Center\nDiscovEHR                          Electronic health record-linked genome study of participants in Geisinger’s         http://www.discovehrshare.com/                                     USA             [21]\n                                   MyCode Community Health Initiative\neMERGE                             Consortium of biorepositories with electronic medical record systems and            https://www.genome.gov/Funded-Programs-Projects/Electronic-        USA             [22]\n                                   genomic information                                                                 Medical-Records-and-Genomics-Network-eMERGE\nKaiser Permanente Research Bank Nationwide biobank collecting genetic information from a blood sample,                 https://researchbank.kaiserpermanente.org/                         USA             [23]\n                                   medical record information, and survey data on lifestyle from seven areas of US\nMillion Veteran Program            Genome cohort study and biobank of participants of the Department of Veterans       https://www.research.va.gov/mvp/                                   USA             [24]\n                                   Affairs (VA) health care system\nCARTaGENE                          Biobank study of 43,000 Québec residents                                            https://www.cartagene.qc.ca/en/home                                Canada          [25]\nlifelines                          Multigenerational cohort study that includes over 167,000 participants from the     https://www.lifelines.nl/                                          Netherlands     [26]\n                                   northern population of the Netherlands\nTaiwan Biobank                     Nationwide biobank and genome cohort study of residents in Taiwan                   https://www.twbiobank.org.tw/test_en/index.php                     Taiwan          [27]\nChina Kadoorie Biobank             Genome cohort study of patients with chronic diseases in China                      https://www.ckbiobank.org/site/                                    China           [28]\n\f                                                                                                                   T. Tanjo et al.\n\n\nstudy have enabled the large-scale association studies           in hot or cold storage when the user is accessing the data\nbetween genotypes and phenotypes. Compared with the              from the same cloud region (more details in https://www.\nformer International 1000 Genomes Project, trait informa-        ncbi.nlm.nih.gov/sra/docs/sra-cloud-access-costs/).\ntion for participants is available, and many studies have           The process of storing original sequencing data and\nshared their individual genomic data under controlled            handling BQS in the sequenced reads to reduce sequencing\naccess to protect the individual’s privacy. Notably, varying     data size in clouds are being studied (https://grants.nih.gov/\npolicies to data sharing for controlled access have an impact    grants/guide/notice-ﬁles/NOT-OD-20-108.html). The total\non collaborative studies across regions or countries. UK         size in SRA was 36 petabytes in 2019 and major parts were\nBiobank with nearly 500,000 participants distributes their       consumed by the BQS. A solution is to downsample the\ndata (including individual genomic data) to the approved         BQS by binning. Without the BQS, the size of a typical\nresearch studies, and these distributed data can be analyzed     SRA ﬁle reduces by 60–70%. Thus, another extreme opi-\non the computational platform of each study group while          nion is to remove the BQS for the standard dataset in\nensuring security. Instead, many studies have not adopted        clouds.\nthe ﬂexible data sharing policy like UK Biobank and cur-            The sequenced data with the fastq format is typically\nrently hinder the reusability and collaboration of research-     aligned to the human reference assembly, usually Genome\ners. Sharing the summary statistics is still the predominant     Reference Consortium Human Build 38 (GRCh38) or\nmethod in international collaborations, and many of the          GRCh37, by using bioinformatics tools, such as bwa [6]\nGWAS meta-analyses have been successful in this way.             and bowtie2 [29]. The de-facto standard output format is the\nHowever, there are still barriers to sharing data at the         SAM text format, which stores each fastq read with the\nindividual level, which hinders collaborative research that      chromosomal position and the alignment status, e.g., mis-\nrequires advanced analysis. Discussions on how to share          matches to the bases in the reference sequences to the\ndata in a ﬂexible manner while protecting individual privacy     reference coordinate (if a fastq read is not located in the\nshould continue to take place. One promising direction           reference, the fastq read is stored in the unmapped section)\nmight be the recently proposed cloud-based solution from         (https://samtools.github.io/hts-specs/SAMv1.pdf).        Com-\nUK Biobank (https://www.ukbiobank.ac.uk/2020/08/uk-              monly, this text format was stored as the BGZF compres-\nbiobank-creates-cloud-based-health-data-analysis-platform-       sion format (extended format from a standard gzip fromat),\nto-unleash-the-imaginations-of-the-worlds-best-scientiﬁc-        called BAM format (https://samtools.github.io/hts-specs/\nminds/)                                                          SAMv1.pdf). Recently, the European Bioinformatics Insti-\n                                                                 tute (EBI) proposed the reference sequence, e.g., GRCh37\n                                                                 and GRCh38, based compression format called CRAM\nHow to store and analyze human genome                            [30]. Contrary to BAM, the CRAM has two compression\ndata efﬁciently?                                                 schemes, lossless or lossy format, i.e., downsample the BQS\n                                                                 and offering 40–50% space saving over the alternative\nThe sequencing data once generated, must be stored in a          BAM format with the lossless option (http://www.htslib.\nspeciﬁc format. In the past, various sequencing formats          org/benchmarks/CRAM.html). The dataset for the Interna-\nhave been proposed, e.g., CSFASTA/QUAL format. For-              tional 1000 Genomes Project can be downloaded in the\ntunately, the current de-facto standard is the fastq format,     BAM format (in total 56.4 terabyte for low-coverage phase\nwhich is a text-based format with sequencing bases and the       3 dataset aligned to GRCh37 reference assembly) as well as\nquality score for each base (base quality score, BQS). The       lossy CRAM format with 8-bin compression scheme to\ndeﬁnitions of BQS range are different among vendors and/         reduce the total download size (https://www.internationa\nor versions, e.g., the quality score ranges from 33 (corre-      lgenome.org/category/cram/) compared to the BAM format\nsponds to! in the ascii code table) to 73 (I) in Sanger format   (in total 18.2 terabyte for the same dataset aligned to\nand 64 (@) to 104 (h) in the Solexa format. In the early days    GRCh38DH reference assembly).\nof NGS technology, problems due to variations were not              The aligned sequenced data are then called variants by\nspeculated. The Sequencing Read Archive (SRA) in the             tools to detect variants, e.g., Genome Analysis ToolKit\nNational Center for Biotechnology Information (NCBI) is          (GATK) [31, 32] and Google’s DeepVariant for germline\nresponsible for storing raw sequencing data in the United        variant call [33] and MuTect2 for somatic variant call [34].\nStates (US). For reusability for users, normalized data of the      The alignment and variant call for thousands of WGS\nBQS (quality adjusted to the standardized format) are also       dataset require adequate computational resources.\nstored and distributed from SRA. The data are now shifting       Therefore, to reduce the computation time for these WGS\ntoward public clouds, i.e., Google Cloud Platform (GCP)          analyses, several hardware or software-based solutions\nand Amazon Web Service (AWS). Users have no end-user             have been proposed [35]. The NVIDIA Clara™\ncharges for accessing cloud SRA data in the cloud, whether       Parabricks developed the Graphics Processing Unit\n\fPractical guide for managing large-scale human genome data in research\n\n\n(GPU)-accelerated tools (https://www.parabricks.com/).                   Terra and Cromwell on the GCP are one of the best starting\nThe Illumina DRAGEN™ Platform uses highly reconﬁ-                        points for middle-scale data analysis projects.\ngurable Field-Programmable Gate Array technology                            In addition, since the distributed nature of the cloud is\n(FPGA) to provide the other hardware-accelerated                         especially efﬁcient for large collaborative projects, many\nimplementations of genomic analysis algorithms [36].                     NGS research projects, in particular, the reanalysis of large-\nThe Sentieon analysis pipelines implement software-                      scale archived datasets and large genomics collaborations\nbased optimization algorithms and boost the calculation                  funded by the US agents, are utilizing the cloud computing\nperformance compared with the native tools, such as                      platforms as their primary computational infrastructures\nGATK and MuTect2 [37]. These platforms are available                     [46, 47].\nboth on the on-premises and on the public clouds. The                       Especially, NCBI in National Institutes of Health (NIH)\nstorage cost in public cloud for clinical sequence has been              is now trying to move the computational infrastructure of\ndiscussed by Krumm et al. [38].                                          the comprehensive DNA database toward commercial cloud\n                                                                         platforms. The International Nucleotide Sequence Database\n                                                                         Collaboration (INSDC) that operates among The DNA Data\nWhat kind of computational platforms are                                 Bank of Japan (DDBJ), The European Bioinformatics\nused to store and analyze human genome                                   Institute (EMBL-EBI), and NCBI has been developing\ndata?                                                                    comprehensive DNA sequence databases via DRA, ERA,\n                                                                         and SRA in each region. NCBI is moving SRA data on the\nFor effective genome data sharing and analysis, not only the             GCP and AWS platforms (each about 14PB; https://\nsecurity and legal compliance issues should be addressed,                ncbiinsights.ncbi.nlm.nih.gov/2020/02/24/sra-cloud/)        as\nbut also researchers need to deal with the recent data                   part of the NIH Science and Technology Research Infra-\nexplosions and be familiar with the large-scale computa-                 structure for Discovery, Experimentation, and Sustainability\ntional and networking infrastructures.                                   (STRIDES) Initiative [48].\n   As a solution that addresses both issues, commercial                     On the other hand, cloud computing also has some\ncloud platforms have been gaining attention recently. The                intrinsic real-world problems, such as vendor-lock in,\nworld-leading cloud platforms, e.g., GCP, AWS, and                       unpredictable cost of computing, networking and data sto-\nMicrosoft Azure, are achieving and maintaining compliance                rage, inconsistent security, and multiple management tools.\nwith complex regulatory requirements, frameworks, and                       According to the investigation in July 2018 [49, 50],\nguidelines. This does not mean that the organization pro-                >80% of companies around the world describe their cloud\nviding some services on the cloud platforms will be auto-                strategy as multi-cloud, commonly deﬁned as using multi-\nmatically certiﬁed under those regulations; however,                     ple public and private clouds for different application\nutilizing cloud platforms can make it easier for researchers             workloads. In general, multi-cloud strategy is used for\nto meet the compliance [39–43].                                          making balances between costs, performances, and custo-\n   In addition to the privacy compliance issue, as a con-                mizability. Again, it poses challenges to provide consistent\nsequence of recent data explosion in GWAS and NGS                        infrastructure and easy operations across multiple-cloud\nresearch [44], copying data to the researcher’s on-premise               vendors and on-premise computers. Several cutting-edge\nservers has become increasingly difﬁcult since projects                  computer technologies can be used for these purposes, as\nutilizing thousands of genomes need to operate on several                described later, especially the Linux container technologies\nhundred terabytes of data, which could take months to                    and its federation on some dedicated management middle-\ndownload. Therefore, for large-scale data analysis, data                 ware including Virtual Cloud Provider (VCP) developed by\nvisiting strategy has emerged as a realistic solution where              the National Institute of Information (NII) [51], Kubernetes\ninstead of bringing data to researchers, the researchers                 (https://kubernetes.io/), and Apache Mesos (http://mesos.\noperate on the data where it resides, e.g., data of Inter-               apache.org/).\nnational 1000 Genomes Project are stored on AWS and                         In the INSDC, Europe and Japan can be classiﬁed into\nNIG as described. The data visiting strategy can be                      the multi-cloud strategy. Computational infrastructure,\nimplemented naturally on commercial cloud platforms.                     which supports the analysis and development of these huge\n   Broad Institute provides a GWAS and NGS data analysis                 databases, is also massive. In the DDBJ, the NIG super-\npipeline execution environment called Terra on GCP [45].                 computer system is offered to medical and biological\nTerra allows researchers to execute many analysis work-                  researchers who require large-scale genome data analysis.\nﬂows on the workﬂow engine called Cromwell, and it                       The current system (which started operation in 2019) is\nalso offers a workﬂow reuse and exchange environment                     equipped with about 14,000 cores CPUs with the peak\nfor research reproducibility, without taking the ownership               performance of 1.1 PFLOPS (CPU: 599.8 TFLOPS, GPU\nof the computational infrastructure and its management.                  499.2 TFLOPS); the total storage capacity is 43.8 petabyte,\n\f                                                                                                                      T. Tanjo et al.\n\n\nand each component is interconnected with high-speed                versions might affect the result of data analysis. Second,\nnetwork (100 Gbps InﬁniBand) suitable for large-scale data          efﬁciently executing workﬂows on different computing\nanalysis [52].                                                      resources is difﬁcult because the computational node of data\n   The NIG supercomputer provides 16 GPU nodes that                 processing is sometimes hard-coded in the programming\nallow genome analysis tools, including GATK [32] and                language. For example, a workﬂow written in GNU make\nMutect2 [34], to accelerate more than one order, by using a         cannot utilize several computing nodes simultaneously,\ndedicated analysis system, e.g., Parabricks genome pipeline.        except for combination to batch job systems, because the\nIt also offers large-scale shared memory (12 terabyte in            tool supports the parallel execution solely in a single\ntotal) computer mainly used for de novo genome assembly             computational node. In modern data analysis, researchers\n[52].                                                               can solve these limitations by combining key technologies\n   The security and legal compliance for the personal gen-          in computer science, the container technology, workﬂow\nome analysis environment of the NIG supercomputer is                description languages, and workﬂow engines (also known\nsupervised by the National Bioscience Database Center               as Scientiﬁc Workﬂow Management Systems (SWfMS) or\n(NBDC) in the Japan Science and Technology Agency                   Workﬂow Management Systems (WMS)). The container\n(JST), and the NIG supercomputer is designated as avail-            technology allows deploying the same tools including its\nable server outside of the afﬁliated organization (“Off-pre-        library dependencies to different computational platforms\nmise-server”) in Japan (https://humandbs.biosciencedbc.jp/          while preserving the computational performance. Workﬂow\nen/off-premise-server). The system is connected to the              description languages and workﬂow engines enable\ncommercial cloud vendors including AWS via the SINET5               researchers to separate the description of workﬂow and the\nnetwork system hosted by NII, Japan [53], and on this               physical computational platform that processes the\nplatform, we have developed a multi-cloud infrastructure            workﬂow.\nwith the cooperation among National Institute of Informa-\ntion, Hokkaido University, Tokyo Institute of Technology,\nKyushu University, and National Institute of Genetics.              Containers\n\n                                                                    Containers have been commonly used to publish applica-\nHow to maintain reproducibility, portability,                       tions [55] as well as provide an isolated computing envir-\nand scalability in genome data analysis, and                        onment, e.g., virtual machine [56]. Although several\nwhy is it important?                                                container engines are proposed (https://opencontainers.org/)\n                                                                    [57, 58], essential concepts for users are the same: a con-\nReproducibility of the data analysis results is one of the          tainer image, a container runtime, and a container registry\nmain concerns in the biomedical ﬁeld [51, 54] since the             (many literatures omit the words “image,” “runtime,” and\nversion of applications and conﬁguration to applications            “instance” (explained later) and simply call them “con-\naffect the results. To maintain the reproducibility of the          tainer”), as described below. First, a container image, e.g.,\nexperimental results, in publication, it has become common          Docker image, OCI image (a variant of Docker image)\nto describe each data processing, e.g., version of tools and        (https://opencontainers.org/), and SIF image [57], is a\nconﬁguration to tools, steps of these data processing, and          package that contains all the dependencies including system\ndataset used in the data analysis (e.g., sequencing data            libraries to execute the application. Each container image is\nand phenotypes). These descriptions allow researchers to re-        identiﬁed by its container image ID, e.g., “6b362a9f73eb,”\nconstruct workﬂows (also known as pipelines), consisting            or the pair of container name and its tag name, e.g., “docker/\nof a sequence of data analysis applications, in their               whalesay:latest” (“docker/whalesay” is the container name\nlaboratories. There are several solutions to denote work-           and “latest” is the tag name). A container image can be built\nﬂows. The naive workﬂows are constructed with bare pro-             from the script named “Dockerﬁle” (for Docker images) or\ngramming languages, e.g., Java or Python, or software build         Singularity deﬁnition ﬁle (for SIF images). Users can build\nsystems, e.g., GNU make (https://www.gnu.org/software/              almost the same container image from a given script. Note\nmake/) or SCons (https://scons.org/). Usually, researchers          that only providing the script ﬁle is not enough to build\ndeploy the applications by downloading and/or building the          completely the same container image because the script can\nsource codes by themselves. However, this naive workﬂow             refer to external resources. The strictest solution to use the\nsometimes causes several limitations. First, deploying              same container image is to refer to the unique image by\napplications to every computing resource is difﬁcult                specifying the same container ID that is already published\nbecause of library dependencies, including system libraries,        in the container registry, which is explained later. Second, a\nas well as the versions of compilers or interpreters are to be      container runtime, e.g., Docker engine [59] and Singularity\nconsidered. If the tool still deploys, the libraries of different   [57], is a system to execute tools in a given container image.\n\fPractical guide for managing large-scale human genome data in research\n\n\nAn executed process using a container image is called a                  nodes in batch job schedulers). Using a workﬂow engine,\ncontainer instance. A container runtime provides an isolated             users can execute workﬂows on various computing\nﬁle system using the container image to the container                    resources without changing workﬂow deﬁnitions. A work-\ninstance as if it is dedicated to the host. By using resource            ﬂow description language describes applications and\nisolation features, e.g., namespace in Linux kernel, rather              workﬂow deﬁnitions for workﬂow engines. A tool\nthan hardware emulation, e.g., virtual machines, a container             description includes input parameters, output parameters, a\nruntime can execute a container instance as efﬁciently as the            container image for execution, and an execution command,\nhost process [60]. In the bioinformatics ﬁeld, Docker engine             whereas a workﬂow description includes connections\nand Singularity are widely used for data analysis applica-               between applications and workﬂows. By using workﬂow\ntions. Docker engine is a container runtime that is widely               description languages, users can construct workﬂows\nused for building data analysis environments [51, 61] as                 without taking care of the execution details of workﬂows\nwell as for building educational applications [62]. It sup-              such as how and where workﬂows are executed.\nports Docker images and OCI images. Although it required                     However, it is difﬁcult for users to choose appropriate\nroot privileges for any container manipulations in older                 workﬂow engines from existing 280+ workﬂow engines\nversions, it experimentally supports executing container                 (https://github.com/common-workﬂow-language/common-\nimages in user privileges since version 19.03. Singularity is            workﬂow-language/wiki/Existing-Workﬂow-systems) that\nanother container runtime, especially for HPC ﬁelds. It                  satisfy each demand. Users have to convert workﬂow\nsupports SIF images as well as Docker images. It only                    deﬁnitions to port it to other workﬂow engines manually in\nrequires user privileges and therefore some HPC systems                  general because each workﬂow engine supports only one or\nhave better support for Singularity, e.g., NIG [52]. Finally, a          a few workﬂow description languages; as described later,\ncontainer registry, e.g., DockerHub (https://hub.docker.                 using the Common Workﬂow Language (CWL) or the\ncom/), Quay.io (https://quay.io/), and SingularityHub                    Workﬂow Description Language (WDL) is a good choice to\n(https://singularity-hub.org/), is a repository that stores and          keep portability between workﬂow engines. Furthermore,\npublishes container images. Container images built by other              they have differences in the supported computing resources,\nregistry users can be downloaded from a container registry;              ecosystems, e.g., workﬂow editors, visualizers, reusable\nresearchers can publish their container images in the con-               tools, and workﬂow repositories. To help users choose\ntainer registry. However, when using container images built              appropriate workﬂow engines, we brieﬂy introduce several\nby other registry users, it is important to verify that they do          workﬂow engines and workﬂow description languages,\nnot contain security vulnerabilities. Fortunately, some                  including their ecosystems. For more details, see [63] and\nimages are already veriﬁed by the container registry pro-                literature for each engine and language.\nvider or by the community. For example, DockerHub pro-                       The Galaxy [64] is a workﬂow manager with a web user\nvides veriﬁed images for well-known Linux distributions,                 interface and enables users to execute workﬂows without\nprogramming language environments, and tools. Another                    using a command-line interface. It also provides a GUI\nexample is BioContainers [55], which provides bioinfor-                  workﬂow editor, tool repository, execution history of\nmatics applications that are veriﬁed by the BioContainers                workﬂows, and many other features. It has been mainly\ncommunity. Other types of container images can be veriﬁed                developed by Penn State University and Johns Hopkins\nby checking the script such as “Dockerﬁle” or using                      University since 2005. Although users can build their own\nsecurity scanning tools for containers such as Docker-                   Galaxy server, there is another choice to use public Galaxy\nBench-for-Security         (https://github.com/docker/docker-            servers that service commonly used applications and refer-\nbench-security), Clair (https://github.com/quay/clair), and              ence genomes (https://galaxyproject.org/use/). We can learn\nTrivy (https://github.com/aquasecurity/trivy).                           how to use Galaxy from ofﬁcial training materials (https://\n                                                                         training.galaxyproject.org/training-material/).\n                                                                             The Nextﬂow [65] is another workﬂow engine as well as\nWorkﬂow engines, workﬂow description                                     a domain-speciﬁc language (DSL). Nextﬂow has a Groovy-\nlanguages, and their ecosystems                                          based DSL, as shown in Fig. 1a, and is easy to understand if\n                                                                         users are already familiar with some programming lan-\nA workﬂow engine is a system to execute workﬂows and                     guages. Nextﬂow also has the GUI frontend [66]. It has\ncan encapsulate how a given workﬂow is controlled and                    been developed by the Comparative Bioinformatics group\nexecuted, e.g., the decision of the order of executions of               at the Barcelona Centre for Genomic Regulation (CRG)\napplications and the re-execution of the failed execution                since 2013. A curated set of tool and workﬂow descriptions\nsteps, and how a given workﬂow is executed on the dif-                   can be found at nf-core [56] and DockStore [67].\nferent computing resources where a given workﬂow is                          The WDL (https://openwdl.org/) is a community-\nexecuted (e.g., cloud computing resources and computing                  driven speciﬁcation and is supported by several\n\f                                                                                                                     T. Tanjo et al.\n\n\n\n\nFig. 1 The simple hello world example by using workﬂow description languages: (a) Nextﬂow, (b) WDL, and (c) CWL\n\n\nworkﬂow engines, e.g., Cromwell (https://github.com/                scalability for large-scale data analysis. First, the con-\nbroadinstitute/cromwell), MiniWDL (https://github.com/              tainer technology allows reproducibility of the published\nchanzuckerberg/miniwdl), and dxWDL (https://github.                 results. In a naive workﬂow, when an application is\ncom/dnanexus/dxWDL). WDL was ﬁrst developed by                      installed on the HPC system by an administrator, then the\nthe Broad Institute and is currently developed by the               administrator is responsible for proper working of the\nOpenWDL community (see Fig. 1b). It has been ofﬁcially              application. When the application is installed on the user’s\nsupported on Terra platform by Broad Institute [45]. We             computational environment, then the person who installed\ncan ﬁnd a curated set of tool and workﬂow descriptions at           it, usually, the user, has the responsibility. However, it is\nBioWDL (https://github.com/biowdl) and DockStore [67].              sometimes impossible to install the same version of\nNote that this paper uses the words “WDL” and capita-               application on user’s environment that is installed on HPC\nlized “Workﬂow Description Language” to indicate the                systems due to version conﬂicts between several HPC\nlanguage by OpenWDL community but some literatures                  systems, for example. Conversely, in the case of modern\nuse the same words to indicate a language to describe               workﬂow, the maintainer of the corresponding container\nworkﬂows.                                                           images for the application has the responsibility. There-\n   The CWL (https://w3id.org/cwl/v1.2/) is another                  fore, a user can use the same application between HPC\ncommunity-driven speciﬁcation and has superior portability          systems and his or her computational environment by\nbetween workﬂow engines. It has been supported by over              using the same container image. Second, the combination\n14 workﬂow engines, including alpha stage (https://www.             of a workﬂow description language and workﬂow engines\ncommonwl.org/#Implementations). Although the YAML-                  allows the portability to different computational envir-\nbased syntax (see Fig. 1c) makes it difﬁcult to understand,         onments and the scalability of data analysis that adapts to\nthere have been many systems that assist to read/write tool         the increase of the size of computational resources. Naive\nand workﬂow deﬁnitions, e.g., GUI editor like Rabix                 workﬂows are described in programming languages or\nComposer (https://rabix.io/) (see Fig. 2) and converters            build tools. Therefore, it is nearly impossible to execute\nfrom/to other languages (https://www.commonwl.org/                  workﬂows on different types of computing resources\n#Converters_and_code_generators). A curated set of tool             without modifying the workﬂow description. In the\na",
  "wordCount": 9476,
  "indexed": "2025-09-25T22:38:43.398Z",
  "method": "direct"
}
